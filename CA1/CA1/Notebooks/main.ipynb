{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_dir = \"../\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "import numpy as np\n",
    "import nltk\n",
    "import codecs\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 112014.05it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 115137.76it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 166440.63it/s]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"importing the filess\"\"\"\n",
    "\n",
    "sentences = [x.strip().split() for x in tqdm.tqdm(codecs.open(F'{path_dir}/Data/test_sentences.txt','rU','utf-8').readlines())]\n",
    "housein_sentences = [x.strip().split() for x in tqdm.tqdm(codecs.open(F'{path_dir}/Data/housein_tests.txt','rU','utf-8').readlines())]\n",
    "events = [x.strip().split() for x in tqdm.tqdm(codecs.open(F'{path_dir}/Data/events.txt','rU','utf-8').readlines())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 10422.07it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 19945.74it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 4715.64it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 5409.01it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 5181.35it/s]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"normalizing the text\"\"\"\n",
    "\n",
    "from __future__ import unicode_literals\n",
    "from hazm import *\n",
    "\n",
    "normalizer = Normalizer()\n",
    "# normalizing by sentence to get a better result\n",
    "sentences_normalized_by_sentence = [normalizer.normalize(' '.join(x)) for x in tqdm.tqdm(sentences)]\n",
    "housein_sentences_normalized_by_sentence = [normalizer.normalize(' '.join(x)) for x in tqdm.tqdm(housein_sentences)]\n",
    "# normalizing by words\n",
    "sentences_normalized_by_word = [[normalizer.normalize(y) for y in x.strip().split()] for x in tqdm.tqdm(sentences_normalized_by_sentence)]\n",
    "housein_sentences_normalized_by_word = [[normalizer.normalize(y) for y in x.strip().split()] for x in tqdm.tqdm(housein_sentences_normalized_by_sentence)]\n",
    "\n",
    "events_normalized = [[normalizer.normalize(y) for y in x] for x in tqdm.tqdm(events)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 11697.78it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 84828.62it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 13171.88it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 45590.26it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 14084.30it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 84222.97it/s]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"tokenizing the text\"\"\"\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "sentences_no_punctuation = [tokenizer.tokenize(' '.join(sent)) for sent in sentences_normalized_by_word ]\n",
    "temp_sentences_tokens = [[word_tokenize(sent) for sent in sents] for sents in tqdm.tqdm(sentences_no_punctuation)]\n",
    "sentences_tokens = [[(word[0] if len(word) else '') for word in sent] for sent in tqdm.tqdm(temp_sentences_tokens)]\n",
    "\n",
    "housein_sentences_no_punctuation = [tokenizer.tokenize(' '.join(sent)) for sent in housein_sentences_normalized_by_word ]\n",
    "housein_temp_sentences_tokens = [[word_tokenize(sent) for sent in sents] for sents in tqdm.tqdm(housein_sentences_no_punctuation)]\n",
    "housein_sentences_tokens = [[(word[0] if len(word) else '') for word in sent] for sent in tqdm.tqdm(housein_temp_sentences_tokens)]\n",
    "\n",
    "temp_events_tokens = [[word_tokenize(sent) for sent in sents] for sents in tqdm.tqdm(events_normalized)]\n",
    "events_tokens = [[(word[0] if len(word) else '') for word in sent] for sent in tqdm.tqdm(temp_events_tokens)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 49669.39it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 47127.01it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 72565.81it/s]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"lemmatization of the text\"\"\"\n",
    "\n",
    "lemmatizer = Lemmatizer()\n",
    "\n",
    "sentences_lemmatied = [[lemmatizer.lemmatize(word) for word in sent if len(word) != 0] for sent in tqdm.tqdm(sentences_tokens)]\n",
    "housein_sentences_lemmatied = [[lemmatizer.lemmatize(word) for word in sent if len(word) != 0] for sent in tqdm.tqdm(housein_sentences_tokens)]\n",
    "events_lemmatied = [[lemmatizer.lemmatize(word) for word in sent if len(word) != 0] for sent in tqdm.tqdm(events_tokens)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zeep import Client\n",
    "from requests.auth import HTTPBasicAuth\n",
    "from requests import Session\n",
    "from zeep.transports import Transport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# address of FarsNet's web services\n",
    "wsdl_sense_service = 'http://nlp.sbu.ac.ir:8180/WebAPI/services/SenseService?WSDL'\n",
    "wsdl_synset_service = 'http://nlp.sbu.ac.ir:8180/WebAPI/services/SynsetService?WSDL'\n",
    "\n",
    "\n",
    "# username and token needed for authentication. You can get this token by signing up on http://farsnet.nlp.sbu.ac.ir\n",
    "username = '12356'\n",
    "token = 'd428eab3-3b91-11eb-8a1e-080027d731c1'\n",
    "\n",
    "# connecting client\n",
    "session = Session()\n",
    "session.auth = HTTPBasicAuth(username, token)\n",
    "client_sense_service = Client(wsdl_sense_service, transport=Transport(session=session))\n",
    "client_synset_service = Client(wsdl_synset_service, transport=Transport(session=session))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [02:34<00:00, 17.13s/it]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"getting the synset ids of the lemmatized text\"\"\"\n",
    "\n",
    "sentences_synset_id = [[client_synset_service.service.getSynsetsByWord(token, 'EXACT', word) for word in sent] for sent in tqdm.tqdm(sentences_lemmatied)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [01:17<00:00, 11.11s/it]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"getting the synset ids of the lemmatized text\"\"\"\n",
    "\n",
    "housein_sentences_synset_id = [[client_synset_service.service.getSynsetsByWord(token, 'EXACT', word) for word in sent] for sent in tqdm.tqdm(housein_sentences_lemmatied)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [01:50<00:00, 11.10s/it]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"getting the synset ids of the lemmatized text\"\"\"\n",
    "\n",
    "events_synset_id = [[client_synset_service.service.getSynsetsByWord(token, 'EXACT', word) for word in sent] for sent in tqdm.tqdm(events_lemmatied)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 17229.00it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 19521.36it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 30066.70it/s]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"extracting the synset ids of the lemmatized text\"\"\"\n",
    "\n",
    "sentences_synset_id_list = [[[synset.id for synset in word] for word in sent] for sent in tqdm.tqdm(sentences_synset_id)]\n",
    "housein_sentences_synset_id_list = [[[synset.id for synset in word] for word in sent] for sent in tqdm.tqdm(housein_sentences_synset_id)]\n",
    "events_synset_id_list = [[[synset.id for synset in word] for word in sent] for sent in tqdm.tqdm(events_synset_id)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_similar_events(synset_id_input, events_synset_id_list_input):\n",
    "    \n",
    "    \"\"\"this function compares the id of words in sentences with id of words in events\"\"\"\n",
    "\n",
    "    count_list = []\n",
    "    for event_synset_id_list in events_synset_id_list_input:\n",
    "        count = 0\n",
    "        for word_synset_id_list in event_synset_id_list:\n",
    "            if synset_id_input in word_synset_id_list:\n",
    "                count += 1\n",
    "        count_list.append(count)\n",
    "\n",
    "    return count_list\n",
    "\n",
    "\n",
    "def count_similar_sentences(sentences_synset_id_list_input, events_synset_id_list_input):\n",
    "\n",
    "    \"\"\"this function creates a list of similarity scores which contains similarity between each event and each word\"\"\"\n",
    "\n",
    "    count_list = []\n",
    "    for sentence_synset_id_list in sentences_synset_id_list_input:\n",
    "        count_list_sentence = []\n",
    "        for word_synset_id_list in sentence_synset_id_list:\n",
    "            count_list_word = []\n",
    "            for word_synset_id in word_synset_id_list:\n",
    "                count_list_word.append(count_similar_events(word_synset_id, events_synset_id_list_input))\n",
    "            count_list_sentence.append(count_list_word)\n",
    "        count_list.append(count_list_sentence)\n",
    "\n",
    "    return count_list\n",
    "\n",
    "\n",
    "def calculate_word_event_similarity(similarity_list_input):\n",
    "\n",
    "    \"\"\"this function calculates the similarity score of each word with each event\"\"\"\n",
    "\n",
    "    similarity_list_output = []\n",
    "    for sentence in similarity_list_input:\n",
    "        sentence_similarity_list = []\n",
    "        for word in sentence:\n",
    "            word_similarity_list = np.array([0,0,0,0,0,0,0,0,0,0])\n",
    "            for id in word:\n",
    "                word_similarity_list += id\n",
    "            sentence_similarity_list.append(word_similarity_list)\n",
    "        similarity_list_output.append(sentence_similarity_list)\n",
    "    \n",
    "    return similarity_list_output\n",
    "    \n",
    "\n",
    "def calculate_sentence_event_similarity(similarity_list_input):\n",
    "\n",
    "    \"\"\"this function calculates the similarity score of each sentence with each event\"\"\"\n",
    "    \n",
    "    similarity_list_output = []\n",
    "    for sentence in similarity_list_input:\n",
    "        sentence_similarity_list = np.array([0,0,0,0,0,0,0,0,0,0])\n",
    "        for word in sentence:\n",
    "            for id in word:\n",
    "                sentence_similarity_list += id\n",
    "        similarity_list_output.append(sentence_similarity_list)\n",
    "    \n",
    "    return similarity_list_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_list = count_similar_sentences(sentences_synset_id_list, events_synset_id_list)\n",
    "housein_similarity_list = count_similar_sentences(housein_sentences_synset_id_list, events_synset_id_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_word_event_similarity = calculate_word_event_similarity(similarity_list)\n",
    "default_sentence_event_similarity = calculate_sentence_event_similarity(similarity_list)\n",
    "\n",
    "housein_word_event_similarity = calculate_word_event_similarity(housein_similarity_list)\n",
    "housein_sentence_event_similarity = calculate_sentence_event_similarity(housein_similarity_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_main_span(word_event_similarity_list_input):\n",
    "\n",
    "    \"\"\"this function finds the top score word of the sentence to find the main span and related desired event\"\"\"\n",
    "\n",
    "    main_span_list = []\n",
    "    for sentence in word_event_similarity_list_input:\n",
    "        main_span_list.append(np.argmax([word[np.argmax(word)] for word in sentence]))\n",
    "    return main_span_list\n",
    "\n",
    "def convert_word_pos_to_span(sentences_main_word_index_input, sentences_input, sentences_tokens_input):\n",
    "\n",
    "    \"\"\"this function converts the word index to its span in the sentence\"\"\"\n",
    "\n",
    "    span_list = []\n",
    "    for i in range(len(sentences_input)):\n",
    "        initial_index = ' '.join(sentences_input[i]).find(sentences_tokens_input[i][sentences_main_word_index_input[i]])\n",
    "        span_list.append([initial_index, initial_index + len(sentences_input[i][sentences_main_word_index_input[i]])])\n",
    "    return span_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 3, 4, 2, 6, 8, 0, 6, 10]\n",
      "[7, 3, 3, 6, 5, 0, 3]\n"
     ]
    }
   ],
   "source": [
    "sentences_main_word_index = find_main_span(default_word_event_similarity)\n",
    "housein_sentences_main_word_index = find_main_span(housein_word_event_similarity)\n",
    "\n",
    "print(sentences_main_word_index)\n",
    "print(housein_sentences_main_word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 8], [11, 18], [21, 26], [11, 17], [27, 33], [40, 46], [0, 5], [33, 38], [51, 54]]\n",
      "[[38, 42], [14, 19], [10, 15], [36, 39], [31, 37], [0, 6], [10, 14]]\n"
     ]
    }
   ],
   "source": [
    "print(convert_word_pos_to_span(sentences_main_word_index, sentences, sentences_tokens))\n",
    "print(convert_word_pos_to_span(housein_sentences_main_word_index, housein_sentences, housein_sentences_tokens))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
