{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_dir = \"../\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 126673.61it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 112061.56it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 158875.15it/s]\n"
     ]
    }
   ],
   "source": [
    "import random \n",
    "import numpy as np\n",
    "import nltk\n",
    "import codecs\n",
    "import tqdm\n",
    "\n",
    "sentences = [x.strip().split() for x in tqdm.tqdm(codecs.open(F'{path_dir}/Data/test_sentences.txt','rU','utf-8').readlines())]\n",
    "housein_sentences = [x.strip().split() for x in tqdm.tqdm(codecs.open(F'{path_dir}/Data/housein_tests.txt','rU','utf-8').readlines())]\n",
    "events = [x.strip().split() for x in tqdm.tqdm(codecs.open(F'{path_dir}/Data/events.txt','rU','utf-8').readlines())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 18696.75it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 12243.59it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 3144.16it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 5326.58it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 5263.28it/s]\n"
     ]
    }
   ],
   "source": [
    "from __future__ import unicode_literals\n",
    "from hazm import *\n",
    "\n",
    "normalizer = Normalizer()\n",
    "# normalizing by sentence to get a better result\n",
    "sentences_normalized_by_sentence = [normalizer.normalize(' '.join(x)) for x in tqdm.tqdm(sentences)]\n",
    "housein_sentences_normalized_by_sentence = [normalizer.normalize(' '.join(x)) for x in tqdm.tqdm(housein_sentences)]\n",
    "# normalizing by words\n",
    "sentences_normalized_by_word = [[normalizer.normalize(y) for y in x.strip().split()] for x in tqdm.tqdm(sentences_normalized_by_sentence)]\n",
    "housein_sentences_normalized_by_word = [[normalizer.normalize(y) for y in x.strip().split()] for x in tqdm.tqdm(housein_sentences_normalized_by_sentence)]\n",
    "\n",
    "events_normalized = [[normalizer.normalize(y) for y in x] for x in tqdm.tqdm(events)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 9994.37it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 78316.88it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 11240.48it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 102300.10it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 17119.61it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 94679.55it/s]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "sentences_no_punctuation = [tokenizer.tokenize(' '.join(sent)) for sent in sentences_normalized_by_word ]\n",
    "temp_sentences_tokens = [[word_tokenize(sent) for sent in sents] for sents in tqdm.tqdm(sentences_no_punctuation)]\n",
    "sentences_tokens = [[(word[0] if len(word) else '') for word in sent] for sent in tqdm.tqdm(temp_sentences_tokens)]\n",
    "\n",
    "housein_sentences_no_punctuation = [tokenizer.tokenize(' '.join(sent)) for sent in housein_sentences_normalized_by_word ]\n",
    "housein_temp_sentences_tokens = [[word_tokenize(sent) for sent in sents] for sents in tqdm.tqdm(housein_sentences_no_punctuation)]\n",
    "housein_sentences_tokens = [[(word[0] if len(word) else '') for word in sent] for sent in tqdm.tqdm(housein_temp_sentences_tokens)]\n",
    "\n",
    "temp_events_tokens = [[word_tokenize(sent) for sent in sents] for sents in tqdm.tqdm(events_normalized)]\n",
    "events_tokens = [[(word[0] if len(word) else '') for word in sent] for sent in tqdm.tqdm(temp_events_tokens)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 38440.67it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 39891.48it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 58579.66it/s]\n"
     ]
    }
   ],
   "source": [
    "stemmer = Stemmer()\n",
    "lemmatizer = Lemmatizer()\n",
    "\n",
    "# sentences_stemmed = [[stemmer.stem(word) for word in sent if len(word) != 0] for sent in tqdm.tqdm(sentences_tokens)]\n",
    "sentences_lemmatied = [[lemmatizer.lemmatize(word) for word in sent if len(word) != 0] for sent in tqdm.tqdm(sentences_tokens)]\n",
    "housein_sentences_lemmatied = [[lemmatizer.lemmatize(word) for word in sent if len(word) != 0] for sent in tqdm.tqdm(housein_sentences_tokens)]\n",
    "\n",
    "# events_stemmed = [[stemmer.stem(word) for word in sent if len(word) != 0] for sent in tqdm.tqdm(events_tokens)]\n",
    "events_lemmatied = [[lemmatizer.lemmatize(word) for word in sent if len(word) != 0] for sent in tqdm.tqdm(events_tokens)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zeep import Client\n",
    "from requests.auth import HTTPBasicAuth\n",
    "from requests import Session\n",
    "from zeep.transports import Transport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# address of FarsNet's web services\n",
    "wsdl_sense_service = 'http://nlp.sbu.ac.ir:8180/WebAPI/services/SenseService?WSDL'\n",
    "wsdl_synset_service = 'http://nlp.sbu.ac.ir:8180/WebAPI/services/SynsetService?WSDL'\n",
    "\n",
    "\n",
    "# username and token needed for authentication. You can get this token by signing up on http://farsnet.nlp.sbu.ac.ir\n",
    "username = '12356'\n",
    "token = 'd428eab3-3b91-11eb-8a1e-080027d731c1'\n",
    "\n",
    "# connecting client\n",
    "session = Session()\n",
    "session.auth = HTTPBasicAuth(username, token)\n",
    "client_sense_service = Client(wsdl_sense_service, transport=Transport(session=session))\n",
    "client_synset_service = Client(wsdl_synset_service, transport=Transport(session=session))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [02:34<00:00, 17.13s/it]\n"
     ]
    }
   ],
   "source": [
    "sentences_synset_id = [[client_synset_service.service.getSynsetsByWord(token, 'EXACT', word) for word in sent] for sent in tqdm.tqdm(sentences_lemmatied)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [01:17<00:00, 11.11s/it]\n"
     ]
    }
   ],
   "source": [
    "housein_sentences_synset_id = [[client_synset_service.service.getSynsetsByWord(token, 'EXACT', word) for word in sent] for sent in tqdm.tqdm(housein_sentences_lemmatied)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [01:53<00:00, 11.35s/it]\n"
     ]
    }
   ],
   "source": [
    "events_synset_id = [[client_synset_service.service.getSynsetsByWord(token, 'EXACT', word) for word in sent] for sent in tqdm.tqdm(events_lemmatied)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 17229.00it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 19521.36it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 30066.70it/s]\n"
     ]
    }
   ],
   "source": [
    "sentences_synset_id_list = [[[synset.id for synset in word] for word in sent] for sent in tqdm.tqdm(sentences_synset_id)]\n",
    "housein_sentences_synset_id_list = [[[synset.id for synset in word] for word in sent] for sent in tqdm.tqdm(housein_sentences_synset_id)]\n",
    "events_synset_id_list = [[[synset.id for synset in word] for word in sent] for sent in tqdm.tqdm(events_synset_id)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparing the ids of words in sentences with words in events with\n",
    "\n",
    "def count_similar_events(synset_id_input, events_synset_id_list_input):\n",
    "    count_list = []\n",
    "    for event_synset_id_list in events_synset_id_list_input:\n",
    "        count = 0\n",
    "        for word_synset_id_list in event_synset_id_list:\n",
    "            if synset_id_input in word_synset_id_list:\n",
    "                count += 1\n",
    "        count_list.append(count)\n",
    "\n",
    "    return count_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_similar_sentences(sentences_synset_id_list_input, events_synset_id_list_input):\n",
    "    count_list = []\n",
    "    for sentence_synset_id_list in sentences_synset_id_list_input:\n",
    "        count_list_sentence = []\n",
    "        for word_synset_id_list in sentence_synset_id_list:\n",
    "            count_list_word = []\n",
    "            for word_synset_id in word_synset_id_list:\n",
    "                count_list_word.append(count_similar_events(word_synset_id, events_synset_id_list_input))\n",
    "            count_list_sentence.append(count_list_word)\n",
    "        count_list.append(count_list_sentence)\n",
    "\n",
    "    return count_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_list = count_similar_sentences(sentences_synset_id_list, events_synset_id_list)\n",
    "housein_similarity_list = count_similar_sentences(housein_sentences_synset_id_list, events_synset_id_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_word_event_similarity(similarity_list_input):\n",
    "    similarity_list_output = []\n",
    "    for sentence in similarity_list_input:\n",
    "        sentence_similarity_list = []\n",
    "        for word in sentence:\n",
    "            word_similarity_list = np.array([0,0,0,0,0,0,0,0,0,0])\n",
    "            for id in word:\n",
    "                word_similarity_list += id\n",
    "            sentence_similarity_list.append(word_similarity_list)\n",
    "        similarity_list_output.append(sentence_similarity_list)\n",
    "    \n",
    "    return similarity_list_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sentence_event_similarity(similarity_list_input):\n",
    "    similarity_list_output = []\n",
    "    for sentence in similarity_list_input:\n",
    "        sentence_similarity_list = np.array([0,0,0,0,0,0,0,0,0,0])\n",
    "        for word in sentence:\n",
    "            for id in word:\n",
    "                sentence_similarity_list += id\n",
    "        similarity_list_output.append(sentence_similarity_list)\n",
    "    \n",
    "    return similarity_list_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_word_event_similarity = calculate_word_event_similarity(similarity_list)\n",
    "default_sentence_event_similarity = calculate_sentence_event_similarity(similarity_list)\n",
    "# print(default_word_event_similarity_list)\n",
    "# print(default_sentence_event_similarity)\n",
    "\n",
    "housein_word_event_similarity = calculate_word_event_similarity(housein_similarity_list)\n",
    "housein_sentence_event_similarity = calculate_sentence_event_similarity(housein_similarity_list)\n",
    "# print(housein_word_event_similarity)\n",
    "# print(housein_sentence_event_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_main_span(word_event_similarity_list_input):\n",
    "    main_span_list = []\n",
    "    for sentence in word_event_similarity_list_input:\n",
    "        main_span_list.append(np.argmax([word[np.argmax(word)] for word in sentence]))\n",
    "    return main_span_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 3, 4, 2, 6, 8, 0, 6, 10]\n",
      "[7, 3, 3, 6, 5, 0, 3]\n"
     ]
    }
   ],
   "source": [
    "sentences_main_word_index = find_main_span(default_word_event_similarity)\n",
    "housein_sentences_main_word_index = find_main_span(housein_word_event_similarity)\n",
    "\n",
    "print(sentences_main_word_index)\n",
    "print(housein_sentences_main_word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_word_pos_to_span(sentences_main_word_index_input, sentences_input, sentences_tokens_input):\n",
    "    span_list = []\n",
    "    for i in range(len(sentences_input)):\n",
    "        initial_index = ' '.join(sentences_input[i]).find(sentences_tokens_input[i][sentences_main_word_index_input[i]])\n",
    "        span_list.append([initial_index, initial_index + len(sentences_input[i][sentences_main_word_index_input[i]])])\n",
    "    return span_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[38, 42], [14, 19], [10, 15], [36, 39], [31, 37], [0, 6], [10, 14]]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(convert_word_pos_to_span(sentences_main_word_index, sentences, sentences_tokens))\n",
    "convert_word_pos_to_span(housein_sentences_main_word_index, housein_sentences, housein_sentences_tokens)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
