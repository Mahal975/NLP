{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals\n",
    "from hazm import *\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "def normalize_inputs(sentences_, events_):\n",
    "    normalizer = Normalizer()\n",
    "    sentences_normalized_by_sentence = [normalizer.normalize(' '.join(x)) for x in tqdm.tqdm(sentences_)]\n",
    "    sentences_normalized_by_word = [[normalizer.normalize(y) for y in x.strip().split()] for x in tqdm.tqdm(sentences_normalized_by_sentence)]\n",
    "    events_normalized = [[normalizer.normalize(y) for y in x] for x in tqdm.tqdm(events_)]\n",
    "\n",
    "    return sentences_normalized_by_word, events_normalized\n",
    "\n",
    "\n",
    "def tokenize_inputs(sentences_, events_):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    sentences_no_punctuation = [tokenizer.tokenize(' '.join(sent)) for sent in sentences_ ]\n",
    "    temp_sentences_tokens = [[word_tokenize(sent) for sent in sents] for sents in tqdm.tqdm(sentences_no_punctuation)]\n",
    "    sentences_tokens = [[(word[0] if len(word) else '') for word in sent] for sent in tqdm.tqdm(temp_sentences_tokens)]\n",
    "\n",
    "    temp_events_tokens = [[word_tokenize(sent) for sent in sents] for sents in tqdm.tqdm(events_)]\n",
    "    events_tokens = [[(word[0] if len(word) else '') for word in sent] for sent in tqdm.tqdm(temp_events_tokens)]\n",
    "\n",
    "    return sentences_tokens, events_tokens\n",
    "\n",
    "\n",
    "def lemmatize_input(sentences_, events_):\n",
    "    lemmatizer = Lemmatizer()\n",
    "\n",
    "    sentences_lemmatied = [[lemmatizer.lemmatize(word) for word in sent if len(word) != 0] for sent in tqdm.tqdm(sentences_)]\n",
    "    events_lemmatied = [[lemmatizer.lemmatize(word) for word in sent if len(word) != 0] for sent in tqdm.tqdm(events_)]\n",
    "\n",
    "    return sentences_lemmatied, events_lemmatied    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connecting to farsnet API functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zeep import Client\n",
    "from requests.auth import HTTPBasicAuth\n",
    "from requests import Session\n",
    "from zeep.transports import Transport\n",
    "\n",
    "def connect_to_fars_net(username_, token_):\n",
    "    # address of FarsNet's web services\n",
    "    wsdl_sense_service = 'http://nlp.sbu.ac.ir:8180/WebAPI/services/SenseService?WSDL'\n",
    "    wsdl_synset_service = 'http://nlp.sbu.ac.ir:8180/WebAPI/services/SynsetService?WSDL'\n",
    "\n",
    "\n",
    "    # username and token needed for authentication. You can get this token by signing up on http://farsnet.nlp.sbu.ac.ir\n",
    "    username = username_\n",
    "    token = token_\n",
    "    # token = 'd428eab3-3b91-11eb-8a1e-080027d731c1'\n",
    "\n",
    "    # connecting client\n",
    "    session = Session()\n",
    "    session.auth = HTTPBasicAuth(username, token)\n",
    "    client_sense_service = Client(wsdl_sense_service, transport=Transport(session=session))\n",
    "    client_synset_service = Client(wsdl_synset_service, transport=Transport(session=session))\n",
    "\n",
    "    return client_sense_service, client_synset_service\n",
    "\n",
    "def get_synset_id(sentences_, client_synset_service_, token_):\n",
    "    return [[client_synset_service_.service.getSynsetsByWord(token_, 'EXACT', word) for word in sent] for sent in tqdm.tqdm(sentences_)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding similar event to each sentence functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_similar_events(synset_id_input, events_synset_id_list_input):\n",
    "    \n",
    "    \"\"\"this function compares the id of words in sentences with id of words in events\"\"\"\n",
    "\n",
    "    count_list = []\n",
    "    for event_synset_id_list in events_synset_id_list_input:\n",
    "        count = 0\n",
    "        for word_synset_id_list in event_synset_id_list:\n",
    "            if synset_id_input in word_synset_id_list:\n",
    "                count += 1\n",
    "        count_list.append(count)\n",
    "\n",
    "    return count_list\n",
    "\n",
    "\n",
    "def count_similar_sentences(sentences_synset_id_list_input, events_synset_id_list_input):\n",
    "\n",
    "    \"\"\"this function creates a list of similarity scores which contains similarity between each event and each word\"\"\"\n",
    "\n",
    "    count_list = []\n",
    "    for sentence_synset_id_list in sentences_synset_id_list_input:\n",
    "        count_list_sentence = []\n",
    "        for word_synset_id_list in sentence_synset_id_list:\n",
    "            count_list_word = []\n",
    "            for word_synset_id in word_synset_id_list:\n",
    "                count_list_word.append(count_similar_events(word_synset_id, events_synset_id_list_input))\n",
    "            count_list_sentence.append(count_list_word)\n",
    "        count_list.append(count_list_sentence)\n",
    "\n",
    "    return count_list\n",
    "\n",
    "\n",
    "def calculate_word_event_similarity(similarity_list_input):\n",
    "\n",
    "    \"\"\"this function calculates the similarity score of each word with each event\"\"\"\n",
    "\n",
    "    similarity_list_output = []\n",
    "    for sentence in similarity_list_input:\n",
    "        sentence_similarity_list = []\n",
    "        for word in sentence:\n",
    "            word_similarity_list = np.array([0,0,0,0,0,0,0,0,0,0])\n",
    "            for id in word:\n",
    "                word_similarity_list += id\n",
    "            sentence_similarity_list.append(word_similarity_list)\n",
    "        similarity_list_output.append(sentence_similarity_list)\n",
    "    \n",
    "    return similarity_list_output\n",
    "    \n",
    "\n",
    "def calculate_sentence_event_similarity(similarity_list_input):\n",
    "\n",
    "    \"\"\"this function calculates the similarity score of each sentence with each event\"\"\"\n",
    "    \n",
    "    similarity_list_output = []\n",
    "    for sentence in similarity_list_input:\n",
    "        sentence_similarity_list = np.array([0,0,0,0,0,0,0,0,0,0])\n",
    "        for word in sentence:\n",
    "            for id in word:\n",
    "                sentence_similarity_list += id\n",
    "        similarity_list_output.append(sentence_similarity_list)\n",
    "    \n",
    "    return similarity_list_output\n",
    "\n",
    "\n",
    "def find_main_span(word_event_similarity_list_input):\n",
    "\n",
    "    \"\"\"this function finds the top score word of the sentence to find the main span and related desired event\"\"\"\n",
    "\n",
    "    main_span_list = []\n",
    "    for sentence in word_event_similarity_list_input:\n",
    "        main_span_list.append(np.argmax([word[np.argmax(word)] for word in sentence]))\n",
    "    return main_span_list\n",
    "\n",
    "\n",
    "def convert_word_pos_to_span(sentences_main_word_index_input, sentences_input, sentences_tokens_input):\n",
    "\n",
    "    \"\"\"this function converts the word index to its span in the sentence\"\"\"\n",
    "\n",
    "    span_list = []\n",
    "    for i in range(len(sentences_input)):\n",
    "        initial_index = ' '.join(sentences_input[i]).find(sentences_tokens_input[i][sentences_main_word_index_input[i]])\n",
    "        span_list.append([initial_index, initial_index + len(sentences_input[i][sentences_main_word_index_input[i]])])\n",
    "    return span_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find longest sequence of words functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger = POSTagger(model='../Resources/postagger.model')\n",
    "\n",
    "def longest_noun_sequence(sent_,index_):\n",
    "    tags_=[]\n",
    "    string_=tagger.tag(sent_[index_:] )\n",
    "    for x in  string_:\n",
    "        tags_.append( x[1][:1])\n",
    "    listToStr = ''.join([str(elem) for elem in tags_])   \n",
    "    tmp=0\n",
    "    for i in range(0,len(listToStr)+1):\n",
    "      if listToStr[i:i+1]=='N':\n",
    "        tmp+=1\n",
    "      else:\n",
    "        break\n",
    "\n",
    "    listToStr2 = ' '.join([str(elem) for elem in sent_[:index_]])  \n",
    "    listToStr3 = ' '.join([str(elem) for elem in sent_[:index_+tmp]])  \n",
    "    return [sent_[index_:index_+tmp],[len(listToStr2)+1,len(listToStr3)]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "import numpy as np\n",
    "import nltk\n",
    "import codecs\n",
    "import tqdm\n",
    "\n",
    "def run(input_string_):\n",
    "    \"\"\"getting the sentences and events\"\"\"\n",
    "    path_dir = \"../\"\n",
    "    sentences = [input_string_.strip().split()]\n",
    "    events = [x.strip().split() for x in tqdm.tqdm(codecs.open(F'{path_dir}/Data/events.txt','rU','utf-8').readlines())]\n",
    "\n",
    "    sentences_normalized, events_normalized = normalize_inputs(sentences, events)\n",
    "    sentences_tokenized, events_tokenized = tokenize_inputs(sentences_normalized, events_normalized)\n",
    "    sentences_lemmatized, events_lemmatized = lemmatize_input(sentences_tokenized, events_tokenized)\n",
    "\n",
    "    token = 'd428eb00-3b91-11eb-8a1e-080027d731c1'\n",
    "    username = '987654'\n",
    "    client_sense_service, client_synset_service = connect_to_fars_net(username, token)\n",
    "\n",
    "    \"\"\"getting the synset ids of the lemmatized sentences\"\"\"\n",
    "    sentences_synset_id = get_synset_id(sentences_lemmatized, client_synset_service, token)\n",
    "\n",
    "    \"\"\"getting the synset ids of the lemmatized events\"\"\"\n",
    "    events_synset_id = get_synset_id(events_lemmatized, client_synset_service, token)\n",
    "\n",
    "    \"\"\"extracting the synset ids of the lemmatized texts\"\"\"\n",
    "    sentences_synset_id_list = [[[synset.id for synset in word] for word in sent] for sent in tqdm.tqdm(sentences_synset_id)]\n",
    "    events_synset_id_list = [[[synset.id for synset in word] for word in sent] for sent in tqdm.tqdm(events_synset_id)]\n",
    "\n",
    "    similarity_list = count_similar_sentences(sentences_synset_id_list, events_synset_id_list)\n",
    "\n",
    "    word_event_similarity = calculate_word_event_similarity(similarity_list)\n",
    "    sentence_event_similarity = calculate_sentence_event_similarity(similarity_list)\n",
    "\n",
    "    sentences_main_word_index = find_main_span(word_event_similarity)\n",
    "    related_event = [np.argmax(sentence) for sentence in sentence_event_similarity]\n",
    "    main_word_span = convert_word_pos_to_span(sentences_main_word_index, sentences, sentences_tokenized)\n",
    "\n",
    "    print(\"sentences_main_word_index: \", sentences_main_word_index)\n",
    "    print(\"related_event: \", related_event)\n",
    "    print(\"main_word_span\", main_word_span)\n",
    "\n",
    "    span_list = [longest_noun_sequence(sentences_tokenized[sent], sentences_main_word_index[sent]) for sent in range(len(sentences_tokenized))]\n",
    "    print(' '.join(sentences_tokenized[0])[span_list[0][1][0]:span_list[0][1][1]])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 104335.92it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 2811.20it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 3813.00it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 3669.88it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  6.57it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 16980.99it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 13490.85it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 112147.17it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 11275.01it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 64231.30it/s]\n",
      "100%|██████████| 1/1 [00:12<00:00, 12.03s/it]\n",
      "100%|██████████| 10/10 [02:32<00:00, 15.23s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00, 9218.25it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 22684.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentences_main_word_index:  [3]\n",
      "related_event:  [3]\n",
      "main_word_span [[11, 18]]\n",
      "استعفای نخست وزیر بریتانیا می\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "run(\"دو هفته از استعفای نخست وزیر بریتانیا می گذرد.\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
