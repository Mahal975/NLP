{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_dir = \"../\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "import numpy as np\n",
    "import nltk\n",
    "import codecs\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 82420.82it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 95325.09it/s]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"importing the files\"\"\"\n",
    "\n",
    "# default sentences\n",
    "sentences = [x.strip().split() for x in tqdm.tqdm(codecs.open(F'{path_dir}/Data/test_sentences.txt','rU','utf-8').readlines())]\n",
    "\n",
    "# housein sentences\n",
    "# sentences = [x.strip().split() for x in tqdm.tqdm(codecs.open(F'{path_dir}/Data/housein_tests.txt','rU','utf-8').readlines())]\n",
    "\n",
    "events = [x.strip().split() for x in tqdm.tqdm(codecs.open(F'{path_dir}/Data/events.txt','rU','utf-8').readlines())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals\n",
    "from hazm import *\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "def normalize_inputs(sentences_, events_):\n",
    "    normalizer = Normalizer()\n",
    "    sentences_normalized_by_sentence = [normalizer.normalize(' '.join(x)) for x in tqdm.tqdm(sentences_)]\n",
    "    sentences_normalized_by_word = [[normalizer.normalize(y) for y in x.strip().split()] for x in tqdm.tqdm(sentences_normalized_by_sentence)]\n",
    "    events_normalized = [[normalizer.normalize(y) for y in x] for x in tqdm.tqdm(events_)]\n",
    "\n",
    "    return sentences_normalized_by_word, events_normalized\n",
    "\n",
    "\n",
    "def tokenize_inputs(sentences_, events_):\n",
    "    tokenizer = RegexpTokenizer(r'[\\w|\\u200c]+')\n",
    "    sentences_no_punctuation = [tokenizer.tokenize(' '.join(sent)) for sent in sentences_ ]\n",
    "    temp_sentences_tokens = [[word_tokenize(sent) for sent in sents] for sents in tqdm.tqdm(sentences_no_punctuation)]\n",
    "    sentences_tokens = [[word[0] for word in sent if len(word)] for sent in tqdm.tqdm(temp_sentences_tokens)]\n",
    "\n",
    "    temp_events_tokens = [[word_tokenize(sent) for sent in sents] for sents in tqdm.tqdm(events_)]\n",
    "    events_tokens = [[(word[0] if len(word) else '') for word in sent] for sent in tqdm.tqdm(temp_events_tokens)]\n",
    "\n",
    "    return sentences_tokens, events_tokens\n",
    "\n",
    "\n",
    "def lemmatize_input(sentences_, events_):\n",
    "    lemmatizer = Lemmatizer()\n",
    "\n",
    "    sentences_lemmatied = [[lemmatizer.lemmatize(word) for word in sent if len(word) != 0] for sent in tqdm.tqdm(sentences_)]\n",
    "    events_lemmatied = [[lemmatizer.lemmatize(word) for word in sent if len(word) != 0] for sent in tqdm.tqdm(events_)]\n",
    "\n",
    "    return sentences_lemmatied, events_lemmatied    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connecting to farsnet API functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zeep import Client\n",
    "from requests.auth import HTTPBasicAuth\n",
    "from requests import Session\n",
    "from zeep.transports import Transport\n",
    "\n",
    "def connect_to_fars_net(username_, token_):\n",
    "    # address of FarsNet's web services\n",
    "    wsdl_sense_service = 'http://nlp.sbu.ac.ir:8180/WebAPI/services/SenseService?WSDL'\n",
    "    wsdl_synset_service = 'http://nlp.sbu.ac.ir:8180/WebAPI/services/SynsetService?WSDL'\n",
    "\n",
    "\n",
    "    # username and token needed for authentication. You can get this token by signing up on http://farsnet.nlp.sbu.ac.ir\n",
    "    username = username_\n",
    "    token = token_\n",
    "    # token = 'd428eab3-3b91-11eb-8a1e-080027d731c1'\n",
    "\n",
    "    # connecting client\n",
    "    session = Session()\n",
    "    session.auth = HTTPBasicAuth(username, token)\n",
    "    client_sense_service = Client(wsdl_sense_service, transport=Transport(session=session))\n",
    "    client_synset_service = Client(wsdl_synset_service, transport=Transport(session=session))\n",
    "\n",
    "    return client_sense_service, client_synset_service\n",
    "\n",
    "def get_synset_id(sentences_, client_synset_service_, token_):\n",
    "    return [[client_synset_service_.service.getSynsetsByWord(token_, 'EXACT', word) for word in sent] for sent in tqdm.tqdm(sentences_)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding similar event to each sentence functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_similar_events(synset_id_input, events_synset_id_list_input):\n",
    "    \n",
    "    \"\"\"this function compares the id of words in sentences with id of words in events\"\"\"\n",
    "\n",
    "    count_list = []\n",
    "    for event_synset_id_list in events_synset_id_list_input:\n",
    "        count = 0\n",
    "        for word_synset_id_list in event_synset_id_list:\n",
    "            if synset_id_input in word_synset_id_list:\n",
    "                count += 1\n",
    "        count_list.append(count)\n",
    "\n",
    "    return count_list\n",
    "\n",
    "\n",
    "def count_similar_sentences(sentences_synset_id_list_input, events_synset_id_list_input):\n",
    "\n",
    "    \"\"\"this function creates a list of similarity scores which contains similarity between each event and each word\"\"\"\n",
    "\n",
    "    count_list = []\n",
    "    for sentence_synset_id_list in sentences_synset_id_list_input:\n",
    "        count_list_sentence = []\n",
    "        for word_synset_id_list in sentence_synset_id_list:\n",
    "            count_list_word = []\n",
    "            for word_synset_id in word_synset_id_list:\n",
    "                count_list_word.append(count_similar_events(word_synset_id, events_synset_id_list_input))\n",
    "            count_list_sentence.append(count_list_word)\n",
    "        count_list.append(count_list_sentence)\n",
    "\n",
    "    return count_list\n",
    "\n",
    "\n",
    "def calculate_word_event_similarity(similarity_list_input):\n",
    "\n",
    "    \"\"\"this function calculates the similarity score of each word with each event\"\"\"\n",
    "\n",
    "    similarity_list_output = []\n",
    "    for sentence in similarity_list_input:\n",
    "        sentence_similarity_list = []\n",
    "        for word in sentence:\n",
    "            word_similarity_list = np.array([0,0,0,0,0,0,0,0,0,0])\n",
    "            for id in word:\n",
    "                word_similarity_list += id\n",
    "            sentence_similarity_list.append(word_similarity_list)\n",
    "        similarity_list_output.append(sentence_similarity_list)\n",
    "    \n",
    "    return similarity_list_output\n",
    "    \n",
    "\n",
    "def calculate_sentence_event_similarity(similarity_list_input):\n",
    "\n",
    "    \"\"\"this function calculates the similarity score of each sentence with each event\"\"\"\n",
    "    \n",
    "    similarity_list_output = []\n",
    "    for sentence in similarity_list_input:\n",
    "        sentence_similarity_list = np.array([0,0,0,0,0,0,0,0,0,0])\n",
    "        for word in sentence:\n",
    "            for id in word:\n",
    "                sentence_similarity_list += id\n",
    "        similarity_list_output.append(sentence_similarity_list)\n",
    "    \n",
    "    return similarity_list_output\n",
    "\n",
    "\n",
    "def find_main_span(word_event_similarity_list_input):\n",
    "\n",
    "    \"\"\"this function finds the top score word of the sentence to find the main span and related desired event\"\"\"\n",
    "\n",
    "    main_span_list = []\n",
    "    for sentence in word_event_similarity_list_input:\n",
    "        main_span_list.append(np.argmax([word[np.argmax(word)] for word in sentence]))\n",
    "    return main_span_list\n",
    "\n",
    "\n",
    "def convert_word_pos_to_span(sentences_main_word_index_input, sentences_input, sentences_tokens_input):\n",
    "\n",
    "    \"\"\"this function converts the word index to its span in the sentence\"\"\"\n",
    "\n",
    "    span_list = []\n",
    "    for i in range(len(sentences_input)):\n",
    "        initial_index = ' '.join(sentences_input[i]).find(sentences_tokens_input[i][sentences_main_word_index_input[i]])\n",
    "        span_list.append([initial_index, initial_index + len(sentences_input[i][sentences_main_word_index_input[i]])])\n",
    "    return span_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find longest sequence of words functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger = POSTagger(model='../Resources/postagger.model')\n",
    "\n",
    "def longest_noun_sequence(sent_,index_):\n",
    "    tags_=[]\n",
    "    string_=tagger.tag(sent_[index_:])\n",
    "    for x in string_:\n",
    "        tags_.append( x[1][:1])\n",
    "    listToStr = ''.join([str(elem) for elem in tags_])   \n",
    "    tmp=0\n",
    "    for i in range(0,len(listToStr)+1):\n",
    "      if listToStr[i:i+1]=='N':\n",
    "        tmp+=1\n",
    "      else:\n",
    "        break\n",
    "\n",
    "    listToStr2 = ' '.join([str(elem) for elem in sent_[:index_]])  \n",
    "    listToStr3 = ' '.join([str(elem) for elem in sent_[:index_+tmp]])  \n",
    "    return [sent_[index_:index_+tmp],[len(listToStr2),len(listToStr3)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 12927.65it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 2595.31it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 4293.48it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 10894.30it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 52795.43it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 14217.98it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 85250.08it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 69518.85it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 54755.93it/s]\n"
     ]
    }
   ],
   "source": [
    "sentences_normalized, events_normalized = normalize_inputs(sentences, events)\n",
    "sentences_tokenized, events_tokenized = tokenize_inputs(sentences_normalized, events_normalized)\n",
    "sentences_lemmatized, events_lemmatized = lemmatize_input(sentences_tokenized, events_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = 'd428eb00-3b91-11eb-8a1e-080027d731c1'\n",
    "username = '987654'\n",
    "client_sense_service, client_synset_service = connect_to_fars_net(username, token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [02:17<00:00, 15.23s/it]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"getting the synset ids of the lemmatized sentences\"\"\"\n",
    "\n",
    "sentences_synset_id = get_synset_id(sentences_lemmatized, client_synset_service, token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [02:19<00:00, 13.98s/it]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"getting the synset ids of the lemmatized events\"\"\"\n",
    "\n",
    "events_synset_id = get_synset_id(events_lemmatized, client_synset_service, token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 16950.49it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 29351.32it/s]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"extracting the synset ids of the lemmatized texts\"\"\"\n",
    "\n",
    "sentences_synset_id_list = [[[synset.id for synset in word] for word in sent] for sent in tqdm.tqdm(sentences_synset_id)]\n",
    "events_synset_id_list = [[[synset.id for synset in word] for word in sent] for sent in tqdm.tqdm(events_synset_id)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_list = count_similar_sentences(sentences_synset_id_list, events_synset_id_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_event_similarity = calculate_word_event_similarity(similarity_list)\n",
    "sentence_event_similarity = calculate_sentence_event_similarity(similarity_list)\n",
    "\n",
    "sentences_main_word_index = find_main_span(word_event_similarity)\n",
    "related_event = [np.argmax(sentence) for sentence in sentence_event_similarity]\n",
    "main_word_span = convert_word_pos_to_span(sentences_main_word_index, sentences, sentences_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentences_main_word_index:  [0, 3, 4, 2, 6, 8, 0, 6, 10]\n",
      "related_event:  [4, 3, 2, 8, 1, 9, 0, 5, 7]\n",
      "main_word_span [[0, 8], [11, 18], [21, 26], [11, 17], [27, 33], [40, 46], [0, 5], [-1, 4], [51, 54]]\n"
     ]
    }
   ],
   "source": [
    "print(\"sentences_main_word_index: \", sentences_main_word_index)\n",
    "print(\"related_event: \", related_event)\n",
    "print(\"main_word_span\", main_word_span)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['گفتگوهای', 'صلح', 'اوکراین'], [0, 20]],\n",
       " [['استعفای', 'نخست', 'وزیر', 'بریتانیا'], [10, 37]],\n",
       " [['حمله'], [19, 24]],\n",
       " [['افزایش', 'بهای', 'طلا'], [10, 26]],\n",
       " [['درگذشت', 'امیرکبیر'], [26, 42]],\n",
       " [['واردات', 'میوه'], [39, 51]],\n",
       " [['دیدار', 'پوتین'], [0, 11]],\n",
       " [['تحریم\\u200cهای'], [32, 42]],\n",
       " [['کسب', 'مدال'], [50, 59]]]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "span_list = [longest_noun_sequence(sentences_tokenized[sent], sentences_main_word_index[sent]) for sent in range(len(sentences_tokenized))]\n",
    "span_list"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
