{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lxPlKaJTcXFx",
    "outputId": "a31240a1-c097-4b33-c13a-1517c122690b"
   },
   "outputs": [],
   "source": [
    "!pip install hazm\n",
    "!pip install zeep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "NI3JGvR8b-96"
   },
   "outputs": [],
   "source": [
    "from hazm import *\n",
    "import random \n",
    "import numpy as np\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import codecs\n",
    "import tqdm\n",
    "from __future__ import unicode_literals\n",
    "from zeep import Client\n",
    "from requests.auth import HTTPBasicAuth\n",
    "from requests import Session\n",
    "from zeep.transports import Transport\n",
    "from __future__ import unicode_literals\n",
    "from parstdex import Parstdex\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_loc = '../Resources/postagger.model'\n",
    "path_dir = \"../\"\n",
    "\n",
    "tagger = POSTagger(model=model_loc)\n",
    "\n",
    "all_events_lists = ['قرار ملاقات', 'مرگ','جنگ','عزل و نصب و استعفا و اتخاب','گفت و گو و مذاکرات و توافق','تحریم و رفع تحریم','بر و باخت و تساوی', 'کسب مدال','تغییر قیمت','واردات و صادرات']\n",
    "ilegal = ['TIME', \"EVENT\", \"COMMUNICATION\", \"PERSON\"]\n",
    "\n",
    "alphabet = [\"آ\",\"ا\",\"ب\",\"پ\",\"ت\",\"ث\",\"ج\",\"چ\",\n",
    "            \"ح\",\"خ\",\"د\",\"ذ\",\"ر\",\"ز\",\"ژ\",\"س\",\n",
    "            \"ش\",\"ص\",\"ض\",\"ط\",\"ظ\",\"ع\",\"غ\",\n",
    "            \"ف\",\"ق\",\"ک\",\"گ\",\"ل\",\"م\",\"ن\"\n",
    "            ,\"و\",\"ه\",\"ی\",\"ھ\",\"ئ\",\"أ\",\"ء\",\"ؤ\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_inputs(sentences_, events_ = None):\n",
    "    normalizer = Normalizer()\n",
    "    sentences_normalized_by_sentence = [normalizer.normalize(' '.join(x)) for x in tqdm.tqdm(sentences_)]\n",
    "    sentences_normalized_by_word = [[normalizer.normalize(y) for y in x.strip().split()] for x in tqdm.tqdm(sentences_normalized_by_sentence)]\n",
    "    if events_:\n",
    "        events_normalized = [[normalizer.normalize(y) for y in x] for x in tqdm.tqdm(events_)]\n",
    "        return sentences_normalized_by_word, events_normalized\n",
    "    return sentences_normalized_by_word\n",
    "\n",
    "\n",
    "def tokenize_inputs(sentences_, events_=None):\n",
    "    tokenizer = RegexpTokenizer(r'[\\w|\\u200c]+')\n",
    "    sentences_no_punctuation = [tokenizer.tokenize(' '.join(sent)) for sent in sentences_ ]\n",
    "    temp_sentences_tokens = [[word_tokenize(sent) for sent in sents] for sents in tqdm.tqdm(sentences_no_punctuation)]\n",
    "    sentences_tokens = [[word[0] for word in sent if len(word)] for sent in tqdm.tqdm(temp_sentences_tokens)]\n",
    "    events_tokens = None\n",
    "    if events_:\n",
    "        temp_events_tokens = [[word_tokenize(sent) for sent in sents] for sents in tqdm.tqdm(events_)]\n",
    "        events_tokens = [[(word[0] if len(word) else '') for word in sent] for sent in tqdm.tqdm(temp_events_tokens)]\n",
    "        return sentences_tokens, events_tokens\n",
    "    return sentences_tokens\n",
    "\n",
    "\n",
    "def lemmatize_input(sentences_, events_):\n",
    "    lemmatizer = Lemmatizer()\n",
    "\n",
    "    sentences_lemmatied = [[lemmatizer.lemmatize(word) for word in sent if len(word) != 0] for sent in tqdm.tqdm(sentences_)]\n",
    "    events_lemmatied = [[lemmatizer.lemmatize(word) for word in sent if len(word) != 0] for sent in tqdm.tqdm(events_)]\n",
    "\n",
    "    return sentences_lemmatied, events_lemmatied "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_to_fars_net(username_, token_):\n",
    "    # address of FarsNet's web services\n",
    "    wsdl_sense_service = 'http://nlp.sbu.ac.ir:8180/WebAPI/services/SenseService?WSDL'\n",
    "    wsdl_synset_service = 'http://nlp.sbu.ac.ir:8180/WebAPI/services/SynsetService?WSDL'\n",
    "\n",
    "\n",
    "    # username and token needed for authentication. You can get this token by signing up on http://farsnet.nlp.sbu.ac.ir\n",
    "    username = username_\n",
    "    token = token_\n",
    "    # token = 'd428eab3-3b91-11eb-8a1e-080027d731c1'\n",
    "\n",
    "    # connecting client\n",
    "    session = Session()\n",
    "    session.auth = HTTPBasicAuth(username, token)\n",
    "    client_sense_service = Client(wsdl_sense_service, transport=Transport(session=session))\n",
    "    client_synset_service = Client(wsdl_synset_service, transport=Transport(session=session))\n",
    "\n",
    "    return client_sense_service, client_synset_service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_synset_id(sentences_, client_synset_service_, token_):\n",
    "    return [[client_synset_service_.service.getSynsetsByWord(token_, 'EXACT', word) for word in sent] for sent in tqdm.tqdm(sentences_)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "username='NimaSalem'\n",
    "token='d428ea27-3b91-11eb-8a1e-080027d731c1'\n",
    "\n",
    "client_sense_service, client_synset_service = connect_to_fars_net(username, token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# event type text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_similar_events(synset_id_input, events_synset_id_list_input):\n",
    "    \n",
    "    \"\"\"this function compares the id of words in sentences with id of words in events\"\"\"\n",
    "\n",
    "    count_list = []\n",
    "    for event_synset_id_list in events_synset_id_list_input:\n",
    "        count = 0\n",
    "        for word_synset_id_list in event_synset_id_list:\n",
    "            if synset_id_input in word_synset_id_list:\n",
    "                count += 1\n",
    "        count_list.append(count)\n",
    "\n",
    "    return count_list\n",
    "\n",
    "\n",
    "def count_similar_sentences(sentences_synset_id_list_input, events_synset_id_list_input):\n",
    "\n",
    "    \"\"\"this function creates a list of similarity scores which contains similarity between each event and each word\"\"\"\n",
    "\n",
    "    count_list = []\n",
    "    for sentence_synset_id_list in sentences_synset_id_list_input:\n",
    "        count_list_sentence = []\n",
    "        for word_synset_id_list in sentence_synset_id_list:\n",
    "            count_list_word = []\n",
    "            for word_synset_id in word_synset_id_list:\n",
    "                count_list_word.append(count_similar_events(word_synset_id, events_synset_id_list_input))\n",
    "            count_list_sentence.append(count_list_word)\n",
    "        count_list.append(count_list_sentence)\n",
    "\n",
    "    return count_list\n",
    "\n",
    "\n",
    "def calculate_word_event_similarity(similarity_list_input):\n",
    "\n",
    "    \"\"\"this function calculates the similarity score of each word with each event\"\"\"\n",
    "\n",
    "    similarity_list_output = []\n",
    "    for sentence in similarity_list_input:\n",
    "        sentence_similarity_list = []\n",
    "        for word in sentence:\n",
    "            word_similarity_list = np.array([0,0,0,0,0,0,0,0,0,0])\n",
    "            for id in word:\n",
    "                word_similarity_list += id\n",
    "            sentence_similarity_list.append(word_similarity_list)\n",
    "        similarity_list_output.append(sentence_similarity_list)\n",
    "    \n",
    "    return similarity_list_output\n",
    "    \n",
    "\n",
    "def calculate_sentence_event_similarity(similarity_list_input):\n",
    "\n",
    "    \"\"\"this function calculates the similarity score of each sentence with each event\"\"\"\n",
    "    \n",
    "    similarity_list_output = []\n",
    "    for sentence in similarity_list_input:\n",
    "        sentence_similarity_list = np.array([0,0,0,0,0,0,0,0,0,0])\n",
    "        for word in sentence:\n",
    "            for id in word:\n",
    "                sentence_similarity_list += id\n",
    "        similarity_list_output.append(sentence_similarity_list)\n",
    "    \n",
    "    return similarity_list_output\n",
    "\n",
    "\n",
    "def find_main_span(word_event_similarity_list_input):\n",
    "\n",
    "    \"\"\"this function finds the top score word of the sentence to find the main span and related desired event\"\"\"\n",
    "\n",
    "    main_span_list = []\n",
    "    for sentence in word_event_similarity_list_input:\n",
    "        main_span_list.append(np.argmax([word[np.argmax(word)] for word in sentence]))\n",
    "    return main_span_list\n",
    "\n",
    "\n",
    "def convert_word_pos_to_span(sentences_main_word_index_input, sentences_input, sentences_tokens_input):\n",
    "\n",
    "    \"\"\"this function converts the word index to its span in the sentence\"\"\"\n",
    "\n",
    "    span_list = []\n",
    "    for i in range(len(sentences_input)):\n",
    "        initial_index = ' '.join(sentences_input[i]).find(sentences_tokens_input[i][sentences_main_word_index_input[i]])\n",
    "        span_list.append([initial_index, initial_index + len(sentences_input[i][sentences_main_word_index_input[i]])])\n",
    "    return span_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def longest_noun_sequence(sent_,index_):\n",
    "    tags_=[]\n",
    "    string_=tagger.tag(sent_[index_:])\n",
    "    for x in string_:\n",
    "        tags_.append( x[1][:1])\n",
    "    listToStr = ''.join([str(elem) for elem in tags_])   \n",
    "    tmp=0\n",
    "    for i in range(0,len(listToStr)+1):\n",
    "      if listToStr[i:i+1]=='N':\n",
    "        tmp+=1\n",
    "      else:\n",
    "        break\n",
    "\n",
    "    listToStr2 = ' '.join([str(elem) for elem in sent_[:index_]])  \n",
    "    listToStr3 = ' '.join([str(elem) for elem in sent_[:index_+tmp]])  \n",
    "    return [sent_[index_:index_+tmp],[len(listToStr2),len(listToStr3)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_event(input_string_):\n",
    "    \"\"\"getting the sentences and events\"\"\"\n",
    "    \n",
    "    sentences = [input_string_.strip().split()]\n",
    "    events = [x.strip().split() for x in tqdm.tqdm(codecs.open(F'{path_dir}/Data/events.txt','rU','utf-8').readlines())]\n",
    "\n",
    "    sentences_normalized, events_normalized = normalize_inputs(sentences, events)\n",
    "    sentences_tokenized, events_tokenized = tokenize_inputs(sentences_normalized, events_normalized)\n",
    "    sentences_lemmatized, events_lemmatized = lemmatize_input(sentences_tokenized, events_tokenized)\n",
    "\n",
    "    \"\"\"getting the synset ids of the lemmatized sentences\"\"\"\n",
    "    sentences_synset_id = get_synset_id(sentences_lemmatized, client_synset_service, token)\n",
    "\n",
    "    \"\"\"getting the synset ids of the lemmatized events\"\"\"\n",
    "    events_synset_id = get_synset_id(events_lemmatized, client_synset_service, token)\n",
    "\n",
    "    \"\"\"extracting the synset ids of the lemmatized texts\"\"\"\n",
    "    sentences_synset_id_list = [[[synset.id for synset in word] for word in sent] for sent in tqdm.tqdm(sentences_synset_id)]\n",
    "    events_synset_id_list = [[[synset.id for synset in word] for word in sent] for sent in tqdm.tqdm(events_synset_id)]\n",
    "\n",
    "    similarity_list = count_similar_sentences(sentences_synset_id_list, events_synset_id_list)\n",
    "\n",
    "    word_event_similarity = calculate_word_event_similarity(similarity_list)\n",
    "    sentence_event_similarity = calculate_sentence_event_similarity(similarity_list)\n",
    "\n",
    "    sentences_main_word_index = find_main_span(word_event_similarity)\n",
    "    related_event = [np.argmax(sentence) for sentence in sentence_event_similarity]\n",
    "    main_word_span = convert_word_pos_to_span(sentences_main_word_index, sentences, sentences_tokenized)\n",
    "\n",
    "    span_list = [longest_noun_sequence(sentences_tokenized[sent], sentences_main_word_index[sent]) for sent in range(len(sentences_tokenized))]\n",
    "    \n",
    "    return span_list, related_event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PLACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regex_finder(sentence):\n",
    "    grammar = r\"\"\"\n",
    "      P: {<P|Pe><Ne|N><RES>?<N|AJ|PRO>*(<CONJ><Ne|N><RES>?<N|AJ|PRO>?)*}\n",
    "    \"\"\"\n",
    "    cp = nltk.RegexpParser(grammar)\n",
    "    return cp.parse(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_finds(tagger):\n",
    "    tagger_finds = []\n",
    "    finds = []\n",
    "    \n",
    "    tree=regex_finder(tagger)\n",
    "    for subtree in tree.subtrees():\n",
    "        if subtree.label() == 'P':\n",
    "            tagger_finds.append(subtree.leaves())\n",
    "    for tagger_find in tagger_finds:\n",
    "        find = []\n",
    "        for k in tagger_find[1:]:\n",
    "            find.append(k[0])\n",
    "        finds.append(find)\n",
    "    return finds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_finds(initial_finds):\n",
    "    final_finds = []\n",
    "    for sub in initial_finds:\n",
    "        sences = []\n",
    "        sen = client_synset_service.service.getSynsetsByWord(token, \"EXACT\", sub[0])\n",
    "        for seny in sen:\n",
    "            sences.append(seny['semanticCategory'])\n",
    "        bad = False\n",
    "        if not sences:\n",
    "            bad = True\n",
    "        if 'LOCATION' not in sences:\n",
    "            for senc in sences:\n",
    "                if senc in ilegal:\n",
    "                    bad = True\n",
    "                    break\n",
    "        if not bad:\n",
    "            final_finds.append(sub)\n",
    "    if not final_finds:\n",
    "        return final_finds\n",
    "    final_finds = final_finds[0]\n",
    "    new_list = [final_finds[0]]\n",
    "\n",
    "    for wo in final_finds[1:]:\n",
    "        if tagger.tag([wo])[0][1] == 'N':\n",
    "            s = client_synset_service.service.getSynsetsByWord(token, \"EXACT\", wo)\n",
    "            ss = []\n",
    "            for seny in s:\n",
    "                ss.append(seny['semanticCategory'])\n",
    "            if not any(ev in ss for ev in ['LOCATION', 'ARTIFACT']) and ss:\n",
    "                continue\n",
    "        new_list.append(wo)\n",
    "                \n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_event_place(sent, topic_span):\n",
    "    if not topic_span:\n",
    "        return \"\"\n",
    "    words = tokenize_inputs(normalize_inputs([[sent]]))[0]\n",
    "    taggs = tagger.tag(words)\n",
    "    finds = initial_finds(taggs)\n",
    "    finds = final_finds(finds)\n",
    "    \n",
    "    if finds:\n",
    "        return ' '.join(finds)\n",
    "    return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_leftmost(topic_span,spans):\n",
    "    output = spans[-1]\n",
    "    for i in range(len(spans)-2,-1,-1):\n",
    "        if spans[i][0] <= output[0] and spans[i][1] >= output[1]:\n",
    "            output = [spans[i][0],spans[i][1]]\n",
    "        else:\n",
    "            break\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_the_time(string):\n",
    "    if any(char in string for char in alphabet):\n",
    "        return True\n",
    "    elif \":\" in string or \"/\" in string:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_timedate(sentence, topic_span):\n",
    "    if not topic_span:\n",
    "        return \"\"\n",
    "    \n",
    "    model = Parstdex()\n",
    "    datetimes = model.extract_span(sentence)['datetime']\n",
    "    num_timedates = len(datetimes)\n",
    "    \n",
    "    if num_timedates == 0:\n",
    "        return \"\"\n",
    "    \n",
    "    elif  num_timedates == 1:\n",
    "        if check_the_time(sentence[datetimes[0][0]:datetimes[0][1]]):\n",
    "            return sentence[datetimes[0][0]:datetimes[0][1]]\n",
    "        else:\n",
    "            return \"\"\n",
    "        \n",
    "    if num_timedates > 1 :\n",
    "        right,left = [],[]\n",
    "        for span in datetimes:\n",
    "            if check_the_time(sentence[span[0]:span[1]]):\n",
    "                if span[1] < topic_span[0]:\n",
    "                    right.append(span)\n",
    "                elif span[0] > topic_span[1]:\n",
    "                    left.append(span)\n",
    "        if len(right)!=0:\n",
    "            final_span = find_leftmost(topic_span,right)\n",
    "            return sentence[final_span[0]:final_span[1]]\n",
    "        elif len(left)!=0:\n",
    "            final_span = find_leftmost(topic_span,left)\n",
    "            return sentence[final_span[0]:final_span[1]]\n",
    "        else:\n",
    "            return \"\"\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(sent):\n",
    "    event_dict = {\"type\": None, \"text\": None, \"place\": None, \"time\": None, \"span\": None}\n",
    "\n",
    "    event_text_span_list, event_type_num = get_event(sent)\n",
    "    event_text = ' '.join(event_text_span_list[0][0])\n",
    "    event_span = event_text_span_list[0][1]\n",
    "    \n",
    "    event_dict['text'] = event_text\n",
    "    event_dict['span'] = event_span\n",
    "    event_dict['type'] = all_events_lists[event_type_num[0]]\n",
    "    event_dict[\"time\"] = find_timedate(sent, event_dict['span'])\n",
    "    event_dict[\"place\"] = get_event_place(sent, event_dict['span'])\n",
    "    \n",
    "    \n",
    "    return event_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = ['دو هفته از استعفای نخست وزیر بریتانیا در انگلیس بزرگ می گذرد.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 10/10 [00:00<00:00, 119837.26it/s]\n",
      "100%|███████████████████████████████████████████| 1/1 [00:00<00:00, 1855.89it/s]\n",
      "100%|███████████████████████████████████████████| 1/1 [00:00<00:00, 1281.09it/s]\n",
      "100%|█████████████████████████████████████████| 10/10 [00:00<00:00, 1780.57it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  5.66it/s]\n",
      "100%|██████████████████████████████████████████| 1/1 [00:00<00:00, 18396.07it/s]\n",
      "100%|████████████████████████████████████████| 10/10 [00:00<00:00, 22203.83it/s]\n",
      "100%|███████████████████████████████████████| 10/10 [00:00<00:00, 107271.20it/s]\n",
      "100%|██████████████████████████████████████████| 1/1 [00:00<00:00, 14364.05it/s]\n",
      "100%|████████████████████████████████████████| 10/10 [00:00<00:00, 94466.31it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:14<00:00, 14.19s/it]\n",
      "100%|███████████████████████████████████████████| 10/10 [01:53<00:00, 11.35s/it]\n",
      "100%|███████████████████████████████████████████| 1/1 [00:00<00:00, 6241.52it/s]\n",
      "100%|████████████████████████████████████████| 10/10 [00:00<00:00, 24428.10it/s]\n",
      "100%|███████████████████████████████████████████| 1/1 [00:00<00:00, 9597.95it/s]\n",
      "100%|███████████████████████████████████████████| 1/1 [00:00<00:00, 3463.50it/s]\n",
      "100%|███████████████████████████████████████████| 1/1 [00:00<00:00, 7639.90it/s]\n",
      "100%|███████████████████████████████████████████| 1/1 [00:00<00:00, 9300.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'type': 'عزل و نصب و استعفا و اتخاب', 'text': 'استعفای نخست وزیر بریتانیا', 'place': 'انگلیس بزرگ', 'time': 'دو هفته', 'span': [10, 37]}\n"
     ]
    }
   ],
   "source": [
    "for sent in sents:\n",
    "    print(run(sent))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "NLP_G.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
